## Softmax

​	在预测分类时，假设我们得到三个类别预测值，分别是10、20、30，这时我们取其中的最大值来作为输出类别。

​	Soft max的作用是将预测值映射到为[0,1]取的值，且不会改变预测结果，即最大值为预测结果。计算方式：
$$
Softmax = \frac{exp(o^i)}{\sum_{i=1}^{n}{o^i}}
，其中o^i为预测类别
$$
​	以前面所举例的值计算可得：[1/6，1/3，1/2]，取最大值即1/2所对应的类别与最终的输出结果。

## 交叉熵损失函数

作用：使用交叉熵来度量学习模型分布与训练数据分布的差异。

交叉熵公式：
$$
H(p(x),q(x))=-\sum_{i=1}^{n}p(x)logq(x)\\
其中q(x)为非真实分布（概率），p(x)为真实分布（标签），是样本的真实标签，n为样本个数
$$
在机器学习中，可以改写为：
$$
H(y^i,\hat{y^i})=-\sum_{i=1}^{n}{y^ilog\hat{y^i}}\\其中，y^i,\hat{y^i}分别为真实标签和预测标签。\\在进行多类别预测时，样本只有一个真实标签，即中只有一个1，其余全为0。
$$
在机器学习中，我们希望在训练数据集上学习到的分布P(model)与真实的分布P(real)越接近越好，所以需要交叉熵越小越好。但我们不知道真实的分布P(real)，故希望我们学习到的分布P(model)与训练数据的分布P(train)接近。

## 扩充—信息熵

一个信息的信息量与它的不确定有直接的关系。在判断一件不确定的事时，需要大量的信息，而判断一件较为确定的事件时，只需少量的信息。故，一个事件的信息量与其不确定性有直接关系。

假设一个随机的事件$ x $，其信息量与其概率分布$p(x)$有关，$I(x)$是其单数，且单调。$x,y$为两个独立的事件，包含的信息量为：
$$
I(x,y)=I(x)+I(y)
$$
而两件事使发生的概率为$p(x,y)=p(x)+p(y)$，通过$log(xy)=log(x)+log(y)$可以得出，$p(x)$与$I(x)$存在对数关系，即：
$$
I(x)=-logp(x)，这里取符号是为了保证I(x)大于0
$$
$I(x)$可以描述为随机变量事件发生时所带来的信息量。

当一个信息在传递时所产生的信息量，通过求其期望可以计算出，即
$$
H(X)=-\sum_{i=1}^{n}p(x)logp(x)
$$
$H(X)$为随机变量$x$的熵，它表示随机变量不确定性的度量，是对可能的事件产生的的信息量的期望。

参考：熵详细解析——https://www.cnblogs.com/kyrieng/p/8694705.html



